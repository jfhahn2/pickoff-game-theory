
\documentclass[11pt]{article}

\usepackage[alldates=year, block=space, style=apa]{biblatex}
\setlength\bibitemsep{0.5\baselineskip}
\addbibresource{../bibliography.bib}

\usepackage[margin=1in]{geometry}
\linespread{1.2}

\usepackage[colorlinks, allcolors=blue]{hyperref}

\usepackage{amsmath, amssymb, amsthm, amsfonts, color, enumerate, framed, graphicx, mathrsfs, mathtools, mdframed, natbib, subcaption, tabularx, theoremref, titlesec, verbatim, xfrac, soul, mathtools, wrapfig, tikz, float}

\graphicspath {{../../output/figures/}}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem{lemma}{Lemma}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem*{assumption*}{Assumption}
\newtheorem{assumption}{Assumption}
\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}

\usepackage[colorinlistoftodos]{todonotes}
\usepackage[normalem]{ulem}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\defeq}{\stackrel{\text{def}}{=}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}

\title{The Two-Foot Rule: A game theoretic analysis of the pickoff limit in Major League Baseball}

\begin{document}

  \maketitle

  \begin{abstract}
    In 2023, Major League Baseball (MLB) introduced new several rule changes to incentivize base stealing and more exciting plays. Among these changes, bases became wider (slighly reducing running distance for a stolen base), and the pitcher was limited to three pickoff attempts. The pickoff limit creates an interesting cat-and-mouse game between pitcher and runner because each failed pickoff attempt adds pressure on the pitcher to avoid using the next pickoff attempt. The runner can increase this pressure by extending the distance of their leadoff toward the next base, increasing their chances of successfully stealing the base. Using pitch-by-pitch lead distance data obtained from MLB, we estimate generalized linear mixed-effects models for pickoff and stolen base outcome probabilities given lead distance, context, and player skill. Using these conditional transition probabilities, we estimate optimal lead distance via two separate Markov decision process models: single-agent (runner only) and two-agent (game theoretic). From these results, we present an actionable rule of thumb: the Two-Foot Rule. We recommend that a runner on first base increase their lead distance by two feet after each successive pickoff attempt.
  \end{abstract}

  \section{Introduction}
  \label{sec:introduction}

    On May 17, 2025, the Chicago Cubs hosted the Chicago White Sox in the second game of their Crosstown Series. In the bottom of the first inning, White Sox pitcher Sean Burke faced Cubs batter Michael Bush with two outs and Cubs runner Kyle Tucker on first base. Before the 1-1 pitch, Burke pivoted and threw to first base behind Tucker, who retreated safely before a tag. At the time, Tucker had 10 stolen bases in 10 attempts on the young season, leading Major League Baseball among base stealers who had not been caught \parencite{fangraphs_major_2025}. On the White Sox broadcast, color commentator Steve Stone explained to the audience, ``The only problem is now there's two throws over there, and Tucker's gonna get a much bigger lead.'' On the next pitch, Tucker stole his 11$^{th}$ base of the season, the first Burke had allowed. ``I don't think anybody in the room could have thrown him out on that play,'' said Stone.

    For background, the stolen base is an exciting play in which a runner takes off on the pitch, attempting to reach the next base before the catcher can throw the ball to the base for a fielder to tag the runner out. Major League Baseball (MLB) stolen base totals peaked in the 1980s and 1990s but then waned in the 2000s and 2010s as teams cut back on the play, which generally requires a high success rate in order to have a positive impact on run scoring \parencite{tango_book_2007}. On the micro level, it improves a team's win probability to reduce stolen base attempts that have negative expected outcomes, but on the macro level, the reduction in stolen base attempts affects the entertainment value of the game.

    Acknowledging this and other trends, MLB introduced several rule changes taking effect in the 2023 regular season, with the goal of making the game more exciting \parencite{castrovince_pitch_2023}. With these changes, a pitch clock limits the time between pitches, and there are restrictions on infielder positioning, curtailing defensive shifts. Two changes are particularly relevant to stolen base attempts: Bases are now wider (reducing the distance between first and second base by 4.5 inches), and there are now count limitations on pickoff attempts, a previously unlimited deterrent of stolen bases.
    
    Whether attempting a stolen base or not, runners typically take a {\it lead} from their base, commonly positioning themselves 8--12 feet away from their base, in the direction of the next base. The longer the lead, the shorter the distance need to run on a stolen base attempt. On a pickoff attempt, instead of throwing a pitch, the pitcher will disengage from the pitching rubber and throw the ball behind the runner to a fielder covering the base, attempting to tag the runner out. Under the new rules, if the pitcher disengages from the pitching rubber three times without successfully picking off the runner, then the runner automatically advances to the next base.

    This disengagement limit creates an interesting cat-and-mouse game between the pitcher and the runner. Each unsuccessful pickoff attempt disincentivizes the pitcher from attempting another pickoff, which incentivizes the runner to increase their lead distance. As explained by Stone before the Tucker stolen base against the White Sox, players and coaches understand this intuitively. But how much should the runner extend their lead? And how does the answer change depending on runner speed and other context?
    
    In the present work, we conduct the first public analysis of this game. We model the baseball inning as a Markov decision process (MDP) and solve for (a) the optimal policy for runner lead distance given current pitcher pickoff behavior and (b) the game theoretic equilibrium behavior in the two-player game between runner and pitcher. We present the practically actionable rule-of-thumb that an average runner should increase their lead distance by two feet after each disengagement.

    \subsection{Related work}

    \textcite{downey_pick_2015} previously analyzed pickoff throws and stolen base attempts as a mixed strategy game without using lead distance data. They modeled the two-player game as a simultaneous decision in which the pitcher chooses whether to attempt a pickoff and the runner chooses whether to attempt a stolen base, showing that this theoretical game has a mixed-strategy Nash equilibrium. Under this equilibrium, left-handed pitchers (possessing better pickoff moves than their right-handed counterparts) would attempt fewer pickoffs and see fewer stolen bases attempted against them, a result validated by empirical data. In a later analysis, \textcite{downey_pressure_2019} argued that pitchers can effectively implement a mixed strategy in low-pressure situations but struggle to randomize their decisions as pressure increases, showing a higher auto-correlation for sequential pickoff decisions in high-pressure situations. Both of these analyses covered a game in which pickoff attempts were unlimited.

    Several studies in sports economics literature have examined the extent to which professional athletes play minimax strategy. \textcite{palacios-huerta_professionals_2003} found evidence consistent with minimax equilibrium behavior in the mixed strategy game of the penalty kick, in which the kicker chooses to shoot the ball left or right and the goalkeeper simultaneously chooses to dive left or right for the save. \textcite{palacios-huerta_experientia_2008} followed this with a laboratory experiment featuring professional soccer players and college students simulating the penalty kick decision using cards, showing that the professionals played minimax but the students did not. On the other hand, \textcite{kovash_professionals_2009} found evidence inconsistent with minimax equilibrium behavior in the National Football League (teams called more run plays and fewer pass plays than theoretically optimal) and in MLB (pitchers threw more fastballs than theoretically optimal).

    The Markov process is a very popular model in sports analytics, particularly in baseball, where the game's highly structured progression lends it well to a state space model \parencite{bukiet_markov_1997}. \textcite{hirotsu_using_2002} used a four-state Markov model for soccer to inform substitution decision-making based on how substitutions impact transition probabilities between states. \textcite{hirotsu_markov_2003} used similar methodology to inform substitution (pinch hitting) decision-making in baseball. Recently, \textcite{chan_points_2021} used a Markov model to evaluate team performance in football. More specifically, the MDP has become particularly popular model in recent years for analyzing decision-making in sports, with applications in tennis \parencite{nadimpalli_when_2013}, baseball \parencite{hirotsu_using_2019}, basketball \parencite{sandholtz_markov_2020}, darts \parencite{baird_optimising_2020} and football \parencite{biro_reinforcement_2022}.

    An intermediate result of the present work is a probability model for stolen base attempts and successes (as well as an analysis how those probabilities changed with the 2023 MLB rule changes), a topic which has not received much attention. \textcite{loughin_assessing_2008} estimated linear mixed-effects models for stolen base attempts and successes, using random effects for pitcher and catcher (not the runner, notably) and fixed effects for balls, strikes, outs, score, venue and pitcher hand. They showed that the spread of pitcher effects is slightly greater than the spread of catcher effects. That analysis did not have data on runner sprint speed and catcher arm strength. More recently, \textcite{stanley_modeling_2023} modeled stolen base success probability using both logistic regression and a random forest, including runner sprint speed and catcher arm strength as features but excluding player identities. That analysis was limited to data prior to the 2023 MLB rule changes that incentivized base stealing.
   
    \section{The baseball inning as a two-player stochastic game}
    \label{sec:sequential-model}
    We first describe a single-player setup where only the first-base runner has agency. This is modeled as an infinite-horizon Markov decision process (MDP). We then extend this model to a two-player case where the first-base runner and the pitcher have agencies. The two-player case is modeled as an infinite-horizon zero sum stochastic game. While the single-player case is more simple and restrictive than the two-player case, it still provides valuable and actionable insights for runners because pitchers are unlikely to adopt an optimal policy in the short term. Since the state space for the single- and two-player models are the same, we first describe the (common) state space before explaining the specifics of each model. 

    {\bf State space.} The game state is given by $s = (b, c, d, o)$, where:
    \begin{align*}
      b &\in \{0, 1\}^3 \mbox{ represents which bases are occupied by runners};\\
      c &\in \{0, 1, 2, 3\} \times \{0, 1, 2\} \mbox{ represents the ball-strike count};\\
      d &\in \{0, 1, 2\} \mbox{ represents the number of disengagements already used}; and\\
      o &\in \{0, 1, 2\} \mbox{ represents the number of outs}.
    \end{align*}
    The total number of non-terminal states is $2^3 \times (4 \times 3) \times 3 \times 3 = 864$. There is one terminal state (end of inning), denoted $\Delta$, bringing the total number of states to 865. We use $\mathcal{S}$ to denote the set of all states ( including the terminal state), and $S_t$ the state at play $t$ in a (fixed) innings. Traditionally, the state is defined by the bases occupied and the number of outs (24 non-terminal states) or by those and the ball-strike count (288 non-terminal states) \parencite{bukiet_markov_1997}. Our construction of the state extends them by further including the number of disengagements as a component of the state, thereby allowing us to analyze the game under the new pickoff limit rule. 

    \subsection{Single-player Markov decision process}
    \label{sec:mdp-model}
    This section considers the setup where agency is allowed only for the first-base runner. We model this scenario as a MDP. The other components of this MDP are as follows.

    {\bf State space.} The state space for the MDP is the one described in the beginning of Section \ref{sec:sequential-model}.

    {\bf Action space.} We limit the agency to states in which first base is occupied and the other bases are unoccupied. For such states, the action is the lead distance in feet chosen by the first-base runner from the first base. We formally describe it as follows. Define ${\cal L} \defeq [0,20]$. The action space of the runner, $A_\text{R}(s) \ \forall s \in {\cal S}$, is then defined as
    \begin{align}
    \label{eqn:runner_action_sapce}
        A_\text{R}(s) \defeq \begin{cases}
            {\cal L} \text{ if } s = (b,c,d,o) \text{ and } b = (1,0,0)\\
            \{\delta\} \text{ if } s = (b,c,d,o) \text{ and } b \neq (1,0,0), \text{ or } s = \Delta.        \end{cases}
    \end{align}    
     Restricting the agency to states in which first base is occupied and the other bases are unoccupied, i.e., $b = (1, 0, 0)$, limits our scope to situations in which the runner might steal second base (encompassing 71\% of stolen base attempts in 2023). This obviously excludes the first-and-third scenario ($b = (1, 0, 1)$) in which the first-base runner will often attempt a stolen base to lure a throw from the catcher, giving the third-base runner a chance to steal home.

     {\bf Single-stage reward function.} The single-stage reward function for transitioning from state $s$ to state $s'$ is given by the number of runs scored in the transition. This reward function does not depend on the action taken by the runner, and is given as
     \begin{align}
         \label{eqn:runner_reward}
         r(s'|s,a) \defeq r(s'|s) \ \forall s,s' \in {\cal S}, a \in A_\text{R}(s),
     \end{align}
     where
        \begin{align}
          \label{eqn:common_reward}
          r(s'|s) \defeq  \begin{cases}
              (g(b) + o) - (g(b') + o') + \mathbb{I}\{c' = (0, 0),\, d' = 0\} \ \forall s,s' \in {\cal S} - \{\Delta\}\\
              0 \text{ if } s = \Delta,
          \end{cases} 
        \end{align}
        where $g: \{0, 1\}^3 \rightarrow \{0, 1, 2, 3\}$ counts the number of runners on base. This expression for the single-stage reward function counts the total number of baserunners and outs before and after the transition and subtracts one if the transition corresponds to the end of a plate appearance. For transitions to the terminal end-of-inning state, the number of runs scored may not be uniquely determined by the starting state. To handle this case, we introduce intermediary states which track the number of runs scored on the last play, only for transitions to the terminal end-of-inning state. These intermediary states transition to the terminal end-of-inning state with probability 1.
    
    {\bf Transition probabilities.} The transition probability function $p(s'|s,a)$ depends on the action taken by the runner. We describe in detail our derivation of transition probabilities for the single-player Markov decision process in Section \ref{sec:transition-mdp-model}. In summary form, we use generalized linear mixed-effects models to estimate runner outcome (e.g., unsuccessful pickoff attempt or successful stolen base attempt) probabilities which depend on lead distance, and we use empirical frequencies to estimate state transition probabilities conditional on runner outcome probabilities. The state transition probabilities depend on lead distance only through the runner outcome probabilities. Any state for which the number of baserunners plus the number of outs is at least three (i.e., $g(b) + o \ge 3$) may transition to the absorbing state with positive probability because there is always the possibility of recording enough outs on a single play to end the inning.

    The MDP is then defined by the tuple ${\cal M} \defeq \langle {\cal S}, \{A_\text{R}(s)\}_{s \in {\cal S}}, \{r(s'|s,a)\}_{s \in {\cal S}, a \in A_\text{R}(s)}^{s' \in {\cal S}}, \\  \{p(s'|s,a)\}_{s \in {\cal S}, a \in A_\text{R}(s)}^{s' \in {\cal S}}\rangle$. Using this MDP, the progression of an inning can be captured as follows. At the beginning of a play, the game is in state $s \in {\cal S}$. Upon observing this state, an action $a \in A_\text{R}(s)$ is picked. The game then transitions to state $s'$ with probability $p(s'|s,a)$ and the batting team collects a reward $r(s'|s,a)$. A play then begins from $s'$ and the process repeats. The goal of the batting team is to maximize the expected total rewards collected over the plays in the inning.

   A (deterministic stationary Markovian) policy $\pi$ for the runner is a mapping ${\cal S} \mapsto \cup_{s \in {\cal S}}A_\text{R}(s)$ such that $\pi(s) \in A_\text{R}(s)$. That is, $\pi(s)$ is the action chosen in state $s$. \sout{In other words, a policy is a contingency plan (in this specific context) employed by the batting team.} Denote the set of all policies by $\Pi_\text{R}$. Note that $\Pi_\text{R}$ is uncountable but compact. It is intuitive that an inning in a baseball eventually halts. The following assumption formalizes this intuition.
   \begin{assumption}\label{assum:mdp_halts}
       \normalfont
      The MDP ${\cal M}$ satisfies
      $
      \lim_{t \to \infty}\sup_{\pi \in \Pi, s \in {\cal S}}\mathbb{P}^\pi[S_t = \Delta | S_0 = s] = 1.
      $
   \end{assumption}   
  \noindent The above assumption states that there exists a natural number $T < \infty$ such that every inning halts within $T$ plays with arbitrarily high probability regardless of the policy employed or the starting state of the inning. 
   
   The value of a policy $\pi$ when starting from state $s \in {\cal S}$ equals  
   \begin{equation}
       \label{eqn:mdp_pol_val}
       V^{\pi}(s) \defeq \mathbb{E}^\pi\left[\sum_{t=0}^\infty r(S_{t+1}|S_t) \Big| S_0 = s\right] \ \forall s \in {\cal S}.
   \end{equation}
  That is, the value of the policy is the the expected total runs scored in an inning.  Under Assumption \ref{assum:mdp_halts}, $V^\pi(s)$ exists for all $\pi \in \Pi$ and $s \in {\cal S}$. To see that, since $r(s'|s) \geq 0$ for all $s,s' \in {\cal S}$, we have by the monotone convergence theorem 
  \[
  V^\pi(s) = \mathbb{E}^\pi\left[\sum_{t=0}^\infty r(S_{t+1}|S_t) \Big| S_0 = s\right] = \lim_{T \to \infty} \mathbb{E}^\pi\left[\sum_{t=0}^T r(S_{t+1}|S_t) \Big| S_0 = s\right] \ \forall \pi \in \Pi, s \in {\cal S}.
  \]
   Using Assumption \ref{assum:mdp_halts} and the fact that $\sup_{s,s' \in {\cal S}}\abs{r(s'|s)} < \infty$ (due to finiteness of ${\cal S}$), it is easy to establish that $\lim_{T \to \infty} \mathbb{E}^\pi\left[\sum_{t=0}^T r(S_{t+1}|S_t) \Big| S_0 = s\right]$ exists for all $\pi \in \Pi, s \in {\cal S}$. Furthermore, the limit is uniformly bounded over $\Pi \times {\cal S}$. The value of the MDP is defined as 
   \begin{equation}
        \label{eqn:mdp_val}
       V^*(s) \defeq \max_{\pi \in \Pi_\text{R}}V^\pi(s) \ \forall s \in S.
   \end{equation}
   The function $V^*$, known as the optimal value function, is the maximum expected runs scored starting from state $s \in {\cal S}$. A policy $\pi^*$ such that $V^{\pi^*}(s) = V^*(s) \ \forall s \in {\cal S}$ is called the optimal policy.

   We present the following definitions that will be needed to characterize the optimal value function and the optimal policy.

   \begin{definition}\label{defn:mdp_operators}
    \normalfont
    \begin{enumerate}
        \item For any $\pi \in \Pi_\text{R}$, the evaluation operator $\Phi_\pi:\mathbb{R}^{\abs{{\cal S}}} \to \mathbb{R}^{\abs{{\cal S}}}$ is defined as
        \[
        (\Phi_\pi V)(s) \defeq \mathbb{E}_{s' \sim p(\cdot|s,\pi(s))}\left[r(s'|s) + V(s')\right] \ \forall s \in {\cal S}.
        \]
        \item The optimality operator $\Phi: \mathbb{R}^{\abs{{\cal S}}} \to \mathbb{R}^{\abs{{\cal S}}}$ is defined as
        \[
        (\Phi V)(s) \defeq \max_{a \in A_\text{R}(s)}\mathbb{E}_{s' \sim p(\cdot|s,a)}\left[r(s'|s) + V(s')\right] \ \forall s \in {\cal S}.
        \]
    \end{enumerate}       
   \end{definition}

   \begin{theorem}[\parencite{bertsekas1991analysis}]\label{thm:mdp_optimality_condition}
       \normalfont
      Suppose Assumption \ref{assum:mdp_halts} holds. Then
      \begin{enumerate}
          \item For any policy $\pi$, its value $V^\pi$ is the unique fixed point of the operator $\Phi_\pi$.
          \item The optimal value function $V^*$ is the unique fixed point of the operator $\Phi$. Furthermore, $\pi^*$ is an optimal policy if and only if $\Phi_{\pi^*}V^* = \Phi V^*$.
      \end{enumerate}
   \end{theorem}
% \todo[inline]{The above theorem also requires $p(s'|s,a)$ to be continuous in $a$ for any $s,s' \in S$. This is true for fixed-effects model since $x \mapsto e^x/1+e^x$ is continuous. I do not know about random-effects model. [[SP: $a$ comes into the model as a fixed effect (the random effects are for player identities), so, yes, $p(s'|s,a)$ is continuous in $a$.]]}

The above theorem enables the characterization of $V^*(s)$ as in the following lemma.

\begin{lemma}\label{lem:mdp_value_function_characterization}
    \normalfont
    Suppose Assumption \ref{assum:mdp_halts} holds. Then $V^*$ is the unique solution to the following system of equations.
    \begin{align*}
        V(s) &= \max_{\ell \in {\cal L}}\sum_{s' \in {\cal S}}\left[r(s'|s) + V(s')\right]p(s'|s,\ell) \quad  \text{ if } s = (b,c,d,o) \text{ and } b = (1,0,0)\\
        V(s) &= \sum_{s' \in {\cal S}}\left[r(s'|s) + V(s')\right]p(s'|s,\delta) \quad \text{ if } s = (b,c,d,o) \text{ and } b \neq (1,0,0)\\
        V(s) &= 0 \quad \text{ if } s = \Delta.
    \end{align*}
    \begin{proof}
        Immediate from Theorem \ref{thm:mdp_optimality_condition}.
    \end{proof}
\end{lemma}

An algorithmic device to obtain the optimal policy is value iteration or policy iteration \parencite{bertsekas1991analysis}. We use policy iteration  in this work to compute the optimal policy which is as follows.  Initialize $V^0 \in \mathbb{R}^{\abs{{\cal S}}}$ and $\pi^0 \in \Pi_\text{R}$, and then iteratively obtain $V^k$ and $\pi^k$ by applying the updates
        \begin{align}
          \label{eqn:update-value-single-agent}
          V^{k+1}(s) &= \sum_{s' \in \mathcal{S}} p(s' | s,\, \pi^k(s)) [r(s, s') + V^k(s')] && \forall s \in \mathcal{S}, \mbox{ and}\\
          \label{eqn:update-policy-single-agent}
          \pi^{k+1}(s) &= \argmax_{a \in A_\text{R}(s)} \sum_{s' \in \mathcal{S}} p(s' | s,\, a) [r(s, s') + V^{k+1}(s')] && \forall s \in \mathcal{S}.
        \end{align}
    The iterates generated by the policy iteration satisfy $V^k \uparrow V^*$ and every limit point of $\pi^k$ is an optimal policy \parencite{bertsekas1991analysis}. 

    \subsection{Two-player stochastic game}
    \label{sec:stochastic-game}
    In this section, we extend the single-agent model by allowing the pitcher to choose whether to attempt a pickoff. This is modeled as a two-player zero sum stochastic game, where the runner first chooses a lead distance, and then the pitcher decides whether to attempt a pickoff after observing the runner's lead distance. The components of this stochastic game are as follows.

    {\bf State space.} The state space  is the same as described in the beginning of Section \ref{sec:sequential-model}.

    {\bf Action space.} As in the single-player model of Section \ref{sec:mdp-model}, we limit actions (of runner as well as pitcher) to states in which only the first base is occupied. The (first-base) runner chooses a lead distance, and then the pitcher chooses a pickoff action $p \in \mathcal{P} \defeq \{0,1\}$, where $p = 1$ corresponds to a pickoff attempt and $0$ otherwise. The action space of the runner, $A_\text{R}(s) \ \forall s \in {\cal S}$, is the same as \eqref{eqn:runner_action_sapce}. The pitcher's action space $A_{\text{P}}(s,a_\text{R}) \ \forall s \in {\cal S}, a_\text{R} \in A_\text{R}(s)$ is defined as
    \begin{align}
    \label{eqn:pitcher_action_sapce}
        A_{\text{P}}(s,a_\text{R}) \defeq \begin{cases}
            {\cal P} \text{ if } s = (b,c,d,o) \text{ and } b = (1,0,0), \ a_\text{R} \in A_\text{R}(s) \\
            \{\delta\} \text{ if } s = (b,c,d,o) \text{ and } b \neq (1,0,0), \text{ or } s = \Delta, \ a_\text{R} \in A_\text{R}(s).        \end{cases}
    \end{align} 

    {\bf Single-stage reward function.} The single-stage reward function for transitioning from state $s$ to state $s'$ is given by the number of runs scored in the transition. Like in the single-player model, this reward function does not depend on the action taken by the runner, and is given
     \begin{align}
         \label{eqn:runner_pitcher_reward}
         r(s'|s,a_\text{R},a_\text{P}) \defeq r(s'|s) \ \forall s,s' \in {\cal S}, a_\text{R} \in A_\text{R}(s), a_\text{P} \in A_\text{P}(s,a_\text{R})
     \end{align}
     where $r(s'|s) \forall s,s' \in {\cal S}$ is as defined in \eqref{eqn:common_reward}. For transitions to the terminal end-of-inning state, the number of runs scored may not be uniquely determined by the starting state. To handle this case, we introduce intermediary states which track the number of runs scored on the last play, only for transitions to the terminal end-of-inning state. These intermediary states transition to the terminal end-of-inning state with probability 1.

    {\bf Transition probabilities.} The transition probability function $p(s'|s,a_\text{R},a_\text{P})$ depends on the actions taken by the pitcher and runner. We describe in detail our derivation of transition probabilities for the two-player stochastic game in Section \ref{sec:transition-stochastic-game}. These transition probabilities are the same as for the single-player Markov decision process, except that the runner outcome probabilities are conditioned on the pitcher's pickoff attempt decision, as well as the lead distance. For example, if the pitcher does not attempt a pickoff, then the successful pickoff outcome and the unsuccessful pickoff outcome have probability zero. Any state for which the number of baserunners plus the number of outs is at least three (i.e., $g(b) + o \ge 3$) may transition to the absorbing state with positive probability because there is always the possibility of recording enough outs on a single play to end the inning.

    The stochastic game is then defined by the tuple ${\cal G} \defeq \langle {\cal S}, \{A_\text{R}(s)\}_{s \in {\cal S}}, \{A_\text{P}(s,a_\text{R})\}_{s \in {\cal S}, a_\text{R} \in A_\text{R}(s)},\\ \{r(s'|s,a_\text{R},a_\text{P})\}_{s \in {\cal S}, a_\text{R} \in A_\text{R}(s), a_\text{P} \in A_\text{P}(s,a_\text{R})}^{s' \in {\cal S}}, \{p(s'|s,a_\text{R},a_\text{P})\}_{s \in {\cal S}, a_\text{R} \in A_\text{R}(s), a_\text{P} \in A_\text{P}(s, a_\text{R})}^{s' \in {\cal S}}\rangle$. The progression of an inning is then modeled as follows. At the beginning of a play, the game is in state $s \in {\cal S}$. Upon observing this state, the batting team picks action $a_\text{R} \in A_\text{R}(s)$. The pitcher then picks an action $a_\text{P} \in A_\text{P}(s,a_\text{R})$. The game then transitions to state $s'$ with probability $p(s'|s,a_\text{R},a_\text{P})$ and the batting team collects a reward $r(s'|s,a_\text{R},a_\text{P})$. A play then begins from $s'$ and the process repeats. The goal of the batting team is to maximize the expected total rewards collected over the plays in the inning while the goal of the pitcher is to minimize the expected total rewards obtained by the batting team.

   A (deterministic stationary Markovian) policy $\pi_\text{R}$ for the runner is a mapping ${\cal S} \mapsto \cup_{s \in {\cal S}}A_\text{R}(s)$ such that $\pi_\text{R}(s) \in A_\text{R}(s)$. That is, $\pi_\text{R}(s)$ is the action chosen in state $s$. Similarly, a (deterministic stationary Markovian) policy $\pi_\text{P}$ for the pitcher is a mapping of the form $\{(s,a) \in {\cal S} \times \cup_{s \in {\cal S}}A_\text{R}(s) \vert a \in A_\text{R}(s)\} \ni (x,a) \mapsto A_\text{P}(x,a)$ with the interpretation that $\pi_\text{P}(x,a_\text{R})$ is the action chosen by the pitcher when the state is $s$ and the runner picks the action $a_\text{R}$. Denote the set of all policies for the runner and pitcher by $\Pi_\text{R}$ and $\Pi_\text{P}$, respectively. 
   Note that $\Pi_\text{R}$ and $\Pi_\text{P}$ are uncountable but compact. It is intuitive that an innings in a baseball eventually halts. The following assumption formalizes this intuition.
   \begin{assumption}\label{assum:game_halts}
       \normalfont
      The stochastic game ${\cal G}$ satisfies
      $
      \lim_{t \to \infty}\sup_{\pi_\text{R} \in \Pi_\text{R}, \pi_\text{P} \in \Pi_\text{P}, s \in {\cal S}}\mathbb{P}^{\pi_\text{R}, \pi_\text{P}}[S_t = \Delta | S_0 = s] = 1.$
   \end{assumption}

   The value of the policy pair $(\pi_\text{R},\pi_\text{P}) \in \Pi_\text{R} \times \Pi_\text{P}$ is given by
    \begin{equation}
       \label{eqn:sg_pol_val}
       \widetilde V^{\pi_\text{R},\pi_\text{P}}(s) \defeq \mathbb{E}^{\pi_\text{R},\pi_\text{P}}\left[\sum_{t=0}^\infty r(S_{t+1}|S_t) \Big| S_0 = s\right] \ \forall s \in {\cal S}.
   \end{equation}
   Similar to the single-player case, it follows that Assumption \ref{assum:game_halts} along with the finiteness of $\cal S$ implies that $\widetilde V^{\pi_\text{R},\pi_\text{P}}(s)$ exists for all $(\pi_\text{R},\pi_\text{P}) \in \Pi_\text{R} \times \Pi_\text{P}, s \in {\cal S}$ and is uniformly bounded over $\Pi_\text{R} \times \Pi_\text{P} \times {\cal S}$. The value of the stochastic game is then defined as 
   \begin{equation}
        \label{eqn:sg_val}
       \widetilde V^*(s) \defeq \max_{\pi_\text{R} \in \Pi_\text{R}}\min_{\pi_\text{P} \in \Pi_\text{P}}\widetilde V^{\pi_\text{R},\pi_\text{P}}(s) \ \forall s \in S.
   \end{equation}
   The function $\widetilde V^*$ will be referred to as the value of the game. A policy pair $(\pi_\text{R}^*,\pi_\text{P}^*) \in \Pi_\text{R} \times \Pi_\text{P}$ such that $\widetilde V^{\pi_\text{R}^*,\pi_\text{P}^*}(s) = \widetilde V^*(s) \ \forall s \in {\cal S}$ is called the optimal strategy or equilibrium strategy. Note that $(\pi_\text{R}^*,\pi_\text{P}^*)$ is an optimal strategy if and only if $\widetilde V^{\pi_\text{R},\pi_\text{P}^*}(s) \leq \widetilde V^{\pi_\text{R}^*,\pi_\text{P}^*}(s) \leq \widetilde V^{\pi_\text{R}^*,\pi_\text{P}}(s) \ \forall \pi_\text{R} \in \Pi_\text{R}, \pi_\text{P} \in \Pi_\text{P}, s \in {\cal S}$.

   We present the following definitions that will be needed to characterize the optimal value function and the optimal policy.

   \begin{definition}\label{defn:sg_operators}
    \normalfont
    \begin{enumerate}
        \item For any $\pi_\text{R} \in \Pi_\text{R}$ and $\pi_\text{P} \in \Pi_\text{P}$, the evaluation operators $\widetilde\Phi_{\pi_\text{R}}:\mathbb{R}^{\abs{{\cal S}}} \to \mathbb{R}^{\abs{{\cal S}}}$ and $\widetilde\Phi_{\pi_\text{P}}:\mathbb{R}^{\abs{{\cal S}}} \to \mathbb{R}^{\abs{{\cal S}}}$ are defined as
        \begin{align*}
        (\widetilde\Phi_{\pi_\text{R}} V)(s) &\defeq \min_{a_\text{P} \in A_\text{P}(s,\pi_\text{R}(s))}\mathbb{E}_{s' \sim p(\cdot|s,\pi_\text{R}(s),a_\text{P})}\left[r(s'|s) + V(s')\right] \ \forall s \in {\cal S}\\
        (\widetilde\Phi_{\pi_\text{P}} V)(s) &\defeq \max_{a_\text{R} \in A_\text{R}(s)}\mathbb{E}_{s' \sim p(\cdot|s,a_\text{R},\pi_\text{P}(s,a_\text{R}))}\left[r(s'|s) + V(s')\right] \ \forall s \in {\cal S}.
        \end{align*}
        \item The optimality operator $\widetilde\Phi: \mathbb{R}^{\abs{{\cal S}}} \to \mathbb{R}^{\abs{{\cal S}}}$ is defined as
        \[
        (\widetilde\Phi V)(s) \defeq \max_{a_\text{R} \in A_\text{R}(s)}\min_{a_\text{P} \in A_\text{P}(s,a_\text{R})}\mathbb{E}_{s' \sim p(\cdot|s,a_\text{R},a_\text{P})}\left[r(s'|s) + V(s')\right] \ \forall s \in {\cal S}.
        \]
    \end{enumerate}       
   \end{definition}

   \begin{theorem}[\parencite{patek1999stochastic}]\label{thm:sg_optimality_condition}
       \normalfont
      Suppose Assumption \ref{assum:game_halts} holds. Then the value of the game $\widetilde V^*$ is the unique fixed point of the operator $\widetilde \Phi$. Furthermore, $(\pi_\text{R}^*,\pi_\text{P}^*) \in \Pi_\text{R} \times \Pi_\text{P}$ is an optimal strategy if and only if $\widetilde\Phi_{\pi_\text{R}^*}\widetilde V^* = \widetilde\Phi \widetilde V^* = \widetilde\Phi_{\pi_\text{P}^*}\widetilde V^*$.
   \end{theorem}

   The above theorem enables the characterization of $\widetilde V^*(s)$ as in the following lemma.

\begin{lemma}\label{lem:sg_value_characterization}
    \normalfont
    Suppose Assumption \ref{assum:game_halts} holds. Then $\widetilde V^*$ is the unique solution to the following system of equations.
    \begin{align*}
        V(s) &= \max_{\ell \in {\cal L}}\min_{a \in {\cal P}}\sum_{s' \in {\cal S}}\left[r(s'|s) + V(s')\right]p(s'|s,\ell,a) \quad  \text{ if } s = (b,c,d,o) \text{ and } b = (1,0,0)\\
        V(s) &= \sum_{s' \in {\cal      S}}\left[r(s'|s) + V(s')\right]p(s'|s,\delta,\delta) \quad \text{ if } s = (b,c,d,o) \text{ and } b \neq (1,0,0)\\
        V(s) &=  0 \quad \text{ if } s = \Delta.
    \end{align*}
    \begin{proof}
        Immediate from Theorem \ref{thm:sg_optimality_condition}.
    \end{proof}
\end{lemma}

   An algorithmic device to obtain the optimal policy is value iteration or policy iteration \parencite{patek1999stochastic}. We use policy iteration  in this work to compute the optimal policy which is as follows.  Initialize $V^0 \in \mathbb{R}^{\abs{{\cal S}}}$ and $\pi_\text{R}^0 \in \Pi_\text{R}$, and then iteratively obtain $V^k$ and $\pi_\text{R}^k$ by applying the updates
        \begin{align}
          \label{eqn:update-value-two-agent}
          V^{k+1}(s) &= \min_{a_\text{P} \in A_\text{P}(s,\pi_\text{R}^k(s))} \sum_{s' \in \mathcal{S}} [r(s, s') +  V^k(s')]p(s'|s,\pi_\text{R}^k(s),a_\text{P}) && \forall s \in \mathcal{S}, \mbox{ and}\\
          \label{eqn:update-policy-two-agent}
          \pi_\text{R}^{k+1}(s) &= \argmax_{a_\text{R} \in A_\text{R}(s)} \left\{\min_{a_\text{p} \in A_\text{P}(s,a_\text{R})}\sum_{s' \in \mathcal{S}}[r(s, s') + V^{k+1}(s')] p(s'|s,a_\text{R},a_\text{P})\right\} && \forall s \in \mathcal{S}.
        \end{align}
    The iterates generated by the policy iteration satisfy $V^k \uparrow \widetilde V^*$\parencite{patek1999stochastic}.

  \section{Transition probability model}
  \label{sec:transition-probability-model}

    In this section, we describe how we use empirical data to estimate state transition probabilities which we use for the single-player Markov decision process and the two-player stochastic game described in Section \ref{sec:sequential-model}. Throughou, we use the following notation to describe the data observed at the beginning of each play $i \in \{1, ..., n\}$ (which can be a pitch or a pickoff attempt):
    \begin{align*}
      b_i & \in \{0, 1\}^3 \mbox{ denotes which bases (1B, 2B, 3B, respectively) are occupied;}\\
      (c_i^B, c_i^S) \equiv c_i & \in \{0, 1, 2, 3\} \times \{0, 1, 2\} \mbox{ denotes the ball-strike count;}\\
      d_i & \in \{0, 1, 2\} \mbox{ denotes the number of disengagements already used; and}\\
      o_i & \in \{0, 1, 2\} \mbox{ denotes the number of outs.}
    \end{align*}
    We use $\mathcal{H}^R$, $\mathcal{H}^P$, and $\mathcal{H}^C$ to denote the sets of runners, pitchers and catchers, respectively. On play $i$:
    \begin{align*}
      h_i^R & \in \mathcal{H}^R \mbox{ is the runner;}\\
      h_i^P & \in \mathcal{H}^P \mbox{ is the pitcher;}\\
      h_i^C & \in \mathcal{H}^C \mbox{ is the catcher;}\\
      z_i^R & \in \mathbb{R^+} \mbox{ is the sprint speed of runner $h_i^R$; and}\\
      z_i^C & \in \mathbb{R^+} \mbox{ is the arm strength of catcher $h_i^C$.}
    \end{align*}
    When the play begins, we use the following notation to describe the actions taken by the players:
    \begin{align}
    \label{eqn:runner-outcome}
      \begin{split}
        \ell_i  \in &~ \mathbb{R}^+ \mbox{ denotes the lead distance (in feet) taken by the first-base runner;}\\
        p_i \in &~ \{0, 1\} \mbox{ denotes whether the pitcher attempts (1) or does not attempt (0) a pickoff; and}\\
        r_i \in &~ \{\mbox{P}^+,\, \mbox{P}^-,\, \mbox{S}^+,\, \mbox{S}^-,\, \mbox{N}\} \equiv \mathcal{R} \mbox{ denotes the runner outcome, where}\\
          & \mbox{P}^+  \mbox{ represents a successful pickoff attempt (runner is out);}\\
          & \mbox{P}^-  \mbox{ represents an unsuccessful pickoff attempt (runner is safe);}\\
          & \mbox{S}^+  \mbox{ represents a successful stolen base attempt (runner is safe);}\\
          & \mbox{S}^-  \mbox{ represents an usuccessful stolen base attempt (runner is out); and}\\
          & \mbox{N}~   \mbox{ represents no runner action (a pitch with no stolen base attempt).}
      \end{split}
    \end{align}
    If there is no runner on first base, then $\ell_i$ and $r_i$ are undefined.

    \subsection{Probability model for runner outcomes}
    \label{sec:prob-runner-outcome}

      As itemized in (\ref{eqn:runner-outcome}), there are five possible outcomes for the runner with respect to the run game. To estimate each of these probabilities conditional on player identities, count and lead distance, we model four probabilities. Using $R_i$ to denote the random variable governing the probability distribution over $r_i$,
      \begin{align*}
        \pi_i    &= \mathbb{P}(R_i \in \{\mbox{P}^+,\, \mbox{P}^-\}) \mbox{ is the pickoff attempt probability;}\\
        \pi_i^+  &= \mathbb{P}(R_i \in \{\mbox{P}^+\} \mid R_i \in \{\mbox{P}^+,\, \mbox{P}^-\}) \mbox{ is the pickoff success probability;}\\
        \psi_i   &= \mathbb{P}(R_i \in \{\mbox{S}^+,\, \mbox{S}^-\} \mid R_i \in \{\mbox{S}^+,\, \mbox{S}^-, N\}) \mbox{ is the stolen base attempt probability; and}\\
        \psi_i^+ &= \mathbb{P}(R_i \in \{\mbox{S}^+\} \mid R_i \in \{\mbox{S}^+,\, \mbox{S}^-\}) \mbox{ is the stolen base success probability.}
      \end{align*}
      We estimate each of these probabilities with generalized linear mixed-effects models, implemented in R using the package lme4 \parencite{bates_fitting_2015}.

      \subsubsection{Pickoff attempt probability}
      \label{sec:prob-po-attempt}

        We model $\pi_i$ using a mixed-effects logistic regression with fixed effects for balls, strikes, outs, disengagements (as a categorical), and lead distance; and with a random effect for the pitcher.
        \begin{align}
          \label{eqn:prob-po-attempt}
          \begin{split}
            \log\left(\frac{\pi_i}{1 - \pi_i}\right) &= \alpha + \beta^B c_i^B + \beta^S c_i^S + \beta^O o_i + \beta^D_{d_i} + \beta^L \ell_i + \gamma^P_{h_i^P}\\
            \gamma^P_{h} &\sim \mathcal{N}(0, \sigma^2_P) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^P.
          \end{split}
        \end{align}
        This model has eight fixed, unknown parameters: the intercept $\alpha$; the five slopes $\beta^B$, $\beta^S$, $\beta^O$, $\beta^D$ and $\beta^L$; and the two variance parameters $\sigma^2_R$ and $\sigma^2_P$. The number of random effects is $|\mathcal{H}^R| + |\mathcal{H}^P|$.
        
      \subsubsection{Pickoff success probability}
      \label{sec:prob-po-success}

        We model $\pi_i^+$ using a mixed-effects logistic regression with a fixed effect for lead distance and a random effect for pitcher. These were the only variables for which we found substantive effects.
        \begin{align}
          \label{eqn:prob-po-success}
          \begin{split}
            \log\left(\frac{\pi_i^+}{1 - \pi_i^+}\right) &= \alpha + \beta^L \ell_i + \gamma^P_{h_i^P},\\
            \gamma^P_{h} &\sim \mathcal{N}(0, \sigma^2_P) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^P.
          \end{split}
        \end{align}
        This model has three fixed, unknown parameters: the intercept $\alpha$, the slope $\beta^L$, and the variance parameters $\sigma^2_P$. The number of random effects is $|\mathcal{H}^P|$.

      \subsubsection{Stolen base attempt probability}
      \label{sec:prob-sb-attempt}

        Modeling the runner's decision to attempt a stolen base is complicated. One may think of this as a deterministic decision made by the runner based on the expected run value with and without attempting a steal. Such a model aligns very poorly with real decisions made by runners. We posit that the runner's ability to attempt a stolen base is influenced by external factors such as the runner's perception of the pitcher's body language timing between pitches. For this reason, we model stolen base attempts as a stochastic process rather than a deterministic decision.

        Deliberately, we exclude lead distance from the stolen base attempt model. While lead distance does help explain the probability of a stolen base attempt, this can cause problems with the framing of the runner's decision. When including lead distance, we find situations in which the expected value of a stolen base attempt is negative, and the optimal lead distance is zero feet to minimize the probability of a stolen base attempt. This is a poor reflection of reality because the runner always has the power to choose not to attempt a stolen base. We obtain a more fit-for-purpose model by excluding lead distance.

        We model $\psi_i$ using a mixed-effects logistic regression with fixed effects for balls, strikes, outs, disengagements (as a categorical), runner sprint speed, and catcher arm strength; and with random effects for runner, pitcher and catcher.
        \begin{align}
          \label{eqn:prob-sb-attempt}
          \begin{split}
            \log\left(\frac{\psi_i}{1 - \psi_i}\right) &= \alpha + \beta^B c_i^B + \beta^S c_i^S + \beta^O o_i + \beta^D_{d_i} + (\beta^R z_i^R + \gamma^R_{h_i^R}) + \gamma^P_{h_i^P} + (\beta^C z_i^C + \gamma^C_{h_i^C}),\\
            \gamma^R_{h} &\sim \mathcal{N}(0, \sigma^2_R) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^R,\\
            \gamma^P_{h} &\sim \mathcal{N}(0, \sigma^2_P) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^P,\\
            \gamma^C_{h} &\sim \mathcal{N}(0, \sigma^2_C) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^C.
          \end{split}
        \end{align}
 
        We interpret $(\beta^R z_i^R + \gamma^R_{h_i^R})$ as the effect of runner $h_i^R$ on the attempt probability, combining the fixed effect of the runner's sprint speed with the random effect of their identity. By including a fixed effect for sprint speed, we are effectively regularizing the runner effects toward priors based on their sprint speeds. The interpretation of $(\beta^C z_i^C + \gamma^C_{h_i^C})$ for catchers is similar: Each catcher has an estimated effect regularized toward a prior based on their arm strength.
      
      \subsubsection{Stolen base success probability}
      \label{sec:prob-sb-success}

        We model $\psi_i^+$ using a mixed-effects logistic regression with fixed effects for lead distance, runner sprint speed, and catcher arm strength; and with random effects for runner, pitcher and catcher.
        \begin{align}
          \label{eqn:prob-sb-success}
          \begin{split}
            \log\left(\frac{\psi_i^+}{1 - \psi_i^+}\right) &= \alpha + \beta^L \ell_i + (\beta^R z_i^R + \gamma^R_{h_i^R}) + \gamma^P_{h_i^P} + (\beta^C z_i^C + \gamma^C_{h_i^C}),\\
            \gamma^R_{h} &\sim \mathcal{N}(0, \sigma^2_R) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^R,\\
            \gamma^P_{h} &\sim \mathcal{N}(0, \sigma^2_P) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^P,\\
            \gamma^C_{h} &\sim \mathcal{N}(0, \sigma^2_C) \hspace{4mm} \mbox{\it i.i.d.} \hspace{4mm} \forall h \in \mathcal{H}^C.
          \end{split}
        \end{align}
        As in (\ref{eqn:prob-sb-attempt}), we interpret $(\beta^R z_i^R + \gamma^R_{h_i^R})$ as the runner effect regularized toward a prior based on sprint speed and $(\beta^C z_i^C + \gamma^C_{h_i^C})$ as the catcher effect regularized toward a prior based on arm strength.

    \subsection{Transition probabilities for the single-player Markov decision process}
    \label{sec:transition-mdp-model}

      To derive the transition probability function $p(s'|s,a)$, we use the law of total probability to combine the runner outcome probabilities estimated in this section with the conditional transition probability given the runner outcome:
      \begin{align}
        \label{eqn:transition-mdp-model}
        p(s'|s,\, a) = \sum_{r \in \mathcal{R}} \mathbb{P}(R_i = r \mid S_i = s,\, \ell_i = a) \cdot \mathbb{P}(S_{i+1} = s' \mid S_i = s,\, R_i = r).
      \end{align}
      In this equation, the runner outcome depends on the lead distance, but we assume that given the runner outcome, the state transition is independent of the lead distance.
      
      To estimate the first probability inside the summation of equation (\ref{eqn:transition-mdp-model}), we use the models from Section \ref{sec:prob-runner-outcome}. Specifically, $\mathbb{P}(R_i = r \mid S_i = s,\, \ell_i = a)$ is given by
      \begin{align*}
        \begin{cases}
            \hfil \pi_i(a) \cdot \pi_i^+(a)                             & \mbox{if } r = \mbox{P}^+\\
            \hfil \pi_i(a) \cdot (1 - \pi_i^+(a))                       & \mbox{if } r = \mbox{P}^-\\
            \hfil (1 - \pi_i(a)) \cdot \psi_i \cdot \psi_i^+(a)         & \mbox{if } r = \mbox{S}^+\\
            \hfil (1 - \pi_i(a)) \cdot \psi_i \cdot (1 - \psi_i^+(a))   & \mbox{if } r = \mbox{S}^-\\
            \hfil (1 - \pi_i(a)) \cdot (1 - \psi_i)                        & \mbox{if } r = \mbox{N}\\
        \end{cases}.
      \end{align*}
      Recall that $\psi_i$ does not depend on $a$, as described in Section \ref{sec:prob-sb-attempt}.

      To estimate $\mathbb{P}(S_{i+1} = s' \mid S_i   = s, R_i = r)$, we use conditional empirical frequencies of the ending state $s'$ given the starting state $s$ and the runner outcome $r$. However, we pool across the starting disengagements $d$ when calculating these empirical frequencies. Our assumption is that, given the runner event, the starting disengagements $d$ does not impact the transition probabilities between the reduced states $\tilde s \equiv (b, c, \cdot, o)$ and $\tilde s' \equiv (b', c', \cdot, o')$. By pooling data across different values of $d$, we mitigate small sample size issues when estimating transition probabilities from rare states, such as having a runner on third base with an 0-0 count, two disengagements and zero outs.

      The pooled empirical transition probability conditions on $e = d' - d$ instead of conditioning on $d$ and $d'$. Using $E_{i}$ to denote the random variable governing the probability distribution over $d_{i + 1} - d_i$,
      \begin{align*}                                  
        Q(\tilde s, r, \tilde s', e)                  
          &= \hat{\mathbb{P}}\left(\tilde S_{i+1} =   \tilde s',\, E_i = e \mid \tilde S_i = \tilde s,\, R_i = r\right)\\
          &= \frac
            {\sum_{i=1}^n\mathbb{I}\left\{\tilde s_i = \tilde s,\, r_i = r\right\} \cdot \mathbb{I}\left\{\tilde s_{i+1} = \tilde s',\, d_{i+1} - d_i = e\right\}}
            {\sum_{i=1}^n\mathbb{I}\left\{\tilde s_i = \tilde s,\, r_i = r\right\}}.
      \end{align*}
      Finally, we estimate the full-state transition probabilities as
      \begin{align*}
        \hat{\mathbb{P}}(S_{i+1} = (b', c', d', o') \mid S_i = (b, c, d, o), R_i = r) = Q\big((b, c, \cdot, o),\, r,\, (b', c', \cdot, o'),\, d' - d\big).
      \end{align*}
   
    \subsection{Transition probabilities for the two-player stochastic game}
    \label{sec:transition-stochastic-game}
 
      To derive the transition probability function $p(s'|s,a_\text{R},a_\text{P})$, we modify equation (\ref{eqn:transition-mdp-model}) to condition the runner outcome probabilities on the pitcher's pickoff attempt decision as well as the runner's lead distance decision:
      \begin{align}
        \label{eqn:transition-stochastic-game}
        p(s'|s,\, a) = \sum_{r \in \mathcal{R}} \mathbb{P}(R_i = r \mid S_i = s,\, \ell_i = a_\text{R}, p_i = a_\text{P}) \cdot \mathbb{P}(S_{i+1} = s' \mid S_i = s,\, R_i = r),
      \end{align}
      where $\mathbb{P}(R_i = r \mid S_i = s,\, \ell_i = a_\text{R},\, p_i = a_\text{P})$ is given by
      \begin{align*}
          \begin{cases}
              \hfil a_\text{P} \cdot \pi_i^+(a_\text{R})                             & \mbox{if } r = \mbox{P}^+\\
              \hfil a_\text{P} \cdot (1 - \pi_i^+(a_\text{R}))                       & \mbox{if } r = \mbox{P}^-\\
              \hfil (1 - a_\text{P}) \cdot \psi_i \cdot \psi_i^+(a_\text{R})         & \mbox{if } r = \mbox{S}^+\\
              \hfil (1 - a_\text{P}) \cdot \psi_i \cdot (1 - \psi_i^+(a_\text{R}))   & \mbox{if } r = \mbox{S}^-\\
              \hfil (1 - a_\text{P}) \cdot (1 - \psi_i)                        & \mbox{if } r = \mbox{N}\\
          \end{cases}.
      \end{align*}
      The procedure for estimating $\mathbb{P}(S_{i+1} = s' \mid S_i = s,\, R_i = r)$ is the same as in Section \ref{sec:transition-mdp-model}.

  \section{Numerical Results}

    In this section, we present the results of applying the single-player Markov decision process model and the two-player stochastic game model to real MLB data. Most of the data were downloaded from the publicly available MLB Stats API using the R package sabRmetrics \parencite{powers_sabrmetrics_2025}. The dataset comprises 722,899 plays from the 2022 MLB regular season and 726,868 plays from the 2023 MLB regular season, where each play is either a pitch or a pickoff attempt (2.0\% pickoff attempts in 2022, 1.2\% in 2023). For each play, we observe the identities of the pitcher, catcher and runners involved. Before and after each play, we observe the state of the game, described by the number of outs in the inning; which bases are occupied by runners; the ball-strike count on the batter; and the number of disengagements already used by the pitcher. On each pickoff attempt, we observe whether it is successful (runner is out) or unsuccessful (runner is safe). On each pitch, we observe whether the runner attempts a stolen base; and if so, we observe whether they are successful (runner is safe) or unsuccessful (runner is out). On each pitch terminating a plate appearance (a batting attempt by a batter), we observe the plate appearance outcome, such as strikeout, groundout, single or home run.

    In addition to the play-by-play data, we manually downloaded publicly available player-season level data from Baseball Savant \parencite{baseball_savant_statcast_2024} describing measurable performance characteristics of catchers and runners in each season, both estimated using computer vision on video data from several camera angles. For each catcher-season, we observe {\it arm strength} (miles per hour), which measures how hard the catcher throws the ball. This is defined as the average of the top 10 percent of throw speeds when the catcher is attempting to catch a base stealer. For each runner-season, we observe {\it sprint speed} (feet per second), which measures the runner's top running speed in their fastest one-second window. This is defined as the average of the top 67 percent of peak running speeds on qualified runs: (1) home-to-first runs on weakly hit batted balls and (2) runs of two or more bases on balls hit into play (excluding runs from second base on extra-base hits).

    In addition to the publicly available data described above, we rely on one key piece of proprietary information provided by MLB Advanced Media, estimated using computer vision on video data from several camera angles. For each play, observe each runner's {\it lead distance} (feet), which is how far they are ranging from their base at the time of the pitcher's first movement. This is defined as the distance between the runner's center of mass (projected onto the line between the runner's current base and their next base) and the nearest edge of the runner's current base. When the pitcher throws a pitch, runners will typically extend their lead by several feet from the time of the pitcher's first movement to the time of pitch release (known as taking a secondary lead), which is relevant for runner advancement on batted balls but not on stolen base attempts. Our data include only the lead distance at the time of the pitcher's first movement, not the secondary lead.

    As detailed in Section \ref{sec:introduction}, MLB made several rule changes between the 2022 season and the 2023 season, most notably (for the present work) widening the bases and limiting the number of pickoffs that can be attempted by the pitcher. Figure \ref{fig:leads-overall} illustrates the difference in runner lead distance behavior (from first base) between 2022 and 2023. With zero disengagements in 2023, the distribution of lead distances is very similar to the overall distribution of lead distances in 2022. With each successive disengagement, we observe a positive shift in the distribution of lead distance in the 2023 data. In 2022, the average lead distance was 9.6 feet. In 2023, the average lead distance under zero, one, and two disengagements was 9.6 feet, 10.3 feet and 11.0 feet, respectively.
 
    \begin{figure}[H]
      \centering
      \includegraphics[width = 0.8\textwidth]{leads_overall_light.pdf}
      \caption{
        \it The league-wide distribution of lead distance from first base when second base is unoccupied, comparing 2022 with 2023. The 2023 data are split by prior disengagements because, beginning in 2023, pitchers were limited to three disengagements, influencing pitcher and runner behavior. In 2022, there was no limit on the number of disengagements. The vertical lines mark the means of the distributions.
      }
      \label{fig:leads-overall}
    \end{figure}
      
    \subsection{Runner outcome probability model}

      Figures \ref{fig:prob-pickoff} and \ref{fig:prob-sb-success} illustrate the relationship between lead distance and runner outcome probabilities, as estimated using the mixed-effects logistic regression models detailed in Section \ref{sec:prob-runner-outcome}. For all three models, the direction of the relationship between lead distance and outcome probability is as expected.
      
      Figure \ref{fig:prob-pickoff} illustrates how pickoff attempt probability and success probability vary with distance. We observe that pickoff attempt rates were lower in 2023 than in 2022, especially as the number of prior disengagement increased. The modeled probability remains low even for extremely long leads. We have very limited data on lead distances beyond 14 feet, especially with fewer than two prior disengagements, so extrapolation much beyond 14 feet is likely not reliable. The ball-strike count and the identity of the pitcher also influence the pickoff attempt probability, but the primary drivers are lead distance and the number prior disengagements. We observe that, except for the most skilled pitchers, pickoff success probability is very low within the range covering the vast majority of lead distances. Toward the high end of this range, the success rate increases quickly as lead distance increases. Interestingly, there is a large gap between the 90$^{th}$-percentile pitcher and the median pitcher but a small gap between the median pitcher and the 10$^{th}$-percentile pitcher, demonstrating a right skew in the distribution of pitcher pickoff skill. Earlier exploration indicated that game state and runner identity did not provide predictive value for pickoff success given lead distance.
 
      \begin{figure}[H]
        \centering
        \includegraphics[width = 0.4\textwidth]{prob_po_attempt_light.pdf}
        \includegraphics[width = 0.4\textwidth]{prob_po_success_light.pdf}
        \caption{
          \it Pickoff attempt probability (left) and success probability (right) modeled as functions of lead distance. The left figure corresponds to the model detailed from Section \ref{sec:prob-po-attempt}, assuming 0 balls, 0 strikes, 0 outs and a median pitcher random effect. The right figure corresponds to the model from Section \ref{sec:prob-po-success}, which includes only an intercept, a fixed effect for lead distance and a random effect for pitcher identity.
        }
        \label{fig:prob-pickoff}
      \end{figure}

      The stolen base attempt probability model does not include lead distance, as explained in Section \ref{sec:prob-sb-attempt}. As a result, this model has only a secondary effect on the lead distance policies learned in later sections.

      Figure \ref{fig:prob-sb-success} shows stolen base success probability modeled as function of lead distance, split by season and by runner skill (based on sprint speed and runner random effect). We observe that, conditioned on lead distance, even 10$^{th}$-percentile runners in 2023 had higher success probabilities than the median runner in 2022, which is likely attributable to rule changes making it easier to steal bases in 2023. Within the range covering most lead distances, the difference between the 90$^{th}$-percentile runner and the median runner is just under 5 percentage points of success probability, similar to the difference between the median runner and the 10$^{th}$-percentile runner. Base-out state and {\it battery} (i.e. pitcher and catcher) skill also influence stolen base success probability although they are not shown in this visualization.
      
      \begin{figure}[H]
        \centering
        \includegraphics[width = 0.8\textwidth]{prob_sb_success_light.pdf}
        \caption{
          \it Stolen base success probability modeled as function of lead distance, split by season and runner effect (combining the fixed effect of runner sprint speed and the random effect of runner identity). This figure shows the probability estimated by the model detailed in Section \ref{sec:prob-sb-success}, assuming 0 balls, 0 strikes, 0 outs and a median value for the combined effect of pitcher identity (random effect), catcher arm strength (fixed effect) and catcher identity (random effect).
        }
        \label{fig:prob-sb-success}
      \end{figure}

      A novel aspect of these runner outcome probability models is the combination of fixed effects for measurable player characteristics (catcher arm strength and runner sprint speed) and random effects for player identities. This feature allows us to answer questions such as: How important are these measurable characteristics for player performance outcomes? And to what extent do players over- or under-perform what is expected from their measurable characteristics? The combination of the fixed effect and the random effect gives us an estimate (which we call the {\it combined effect}) of each player's skill, regularized toward what is expected from their measurable characteristics.

      For the stolen base success probability model, Figure \ref{fig:random-effect} shows the relationship between the combined player effect and the corresponding measurable characteristic, for both catchers and runners. We observe that arm strength explains 65\% of the variance in the catcher combined effect and that sprint speed explains 86\% of the variance in the runner combined effect. In other words, sprint speed tells us more about runner skill than arm strength tells us about catcher skill, with regard to stolen base outcomes. Beyond arm strength, the catcher can reduce stolen base success by releasing the ball more quickly or by throwing it more accurately to the target base. Beyond sprint speed, the runner can increase stolen base success by starting to run earlier (i.e., getting a better jump) or by accelerating more quickly to their top speed.
      
      \begin{figure}[H]
        \centering
        \includegraphics[width = 0.49\textwidth]{effect_catcher_light.pdf}
        \includegraphics[width = 0.49\textwidth]{effect_runner_light.pdf}
        \caption{
          \it
          (Left) The relationship between arm strength and catcher combined effect on stolen base success probability (relative to average, on the log-odds scale). The combined effect is the sum of the fixed effect due to arm strength and the catcher random effect.
          (Right) The relationship between sprint speed and runner combined effect on stolen base success probability (relative to average, on the log-odds scale). The combined effect is the sum of the fixed effect due to sprint speed and the runner random effect.
        }
        \label{fig:random-effect}
      \end{figure}

    \subsection{Single-player Markov decision process}

      In the single-agent MDP, the only decision-maker is the runner, and they choose a policy for lead distance as a function of the game state, with the objective of maximizing the expected number of runs in the inning. In practice, the purpose of this MDP is to optimize runner decision-making given current conditions of pitcher behavior, anticipating that pitcher behavior will take some time to react to changes in runner behavior.

      Figure \ref{fig:finding-optimal-lead} illustrates how the runner would choose an optimal lead distance for different numbers of prior disengagements in an example scenario. Short lead distances have very low pickoff probabilities and low stolen base probabilities. As lead distance increases, stolen base probability increases, but so does pickoff probability. For very long lead distances, the run expectancy drops off sharply as the pickoff probability increases sharply. Between those two extremes, the runner strikes a balance to maximize run expectancy. The optimal lead distances for 0, 1 and 2 disengagements are 10.0 feet, 11.5 feet and 14.1 feet, respectively.
      
      \begin{figure}[H]
        \centering
        \includegraphics[width = 0.8\textwidth]{finding_optimal_lead_light.pdf}
        \caption{
          \it Rest-of-inning run expectancy for different choices of lead distance. Three separate curves show the run expectancy for differing numbers of prior disengagements, assuming 0 balls; 0 strikes; 0 outs; third base unoccupied; and median player effects for pitcher, catcher and runner. Run expectancy is defined as the expectation of the value function $V_{\pi^*}$ (evaluated at the resulting state) corresponding to the optimal policy $\pi^*$ defined by (\ref{eqn:run-expectancy}). The vertical line corresponding to each curve marks the value at which the curve is maximized.
        }
        \label{fig:finding-optimal-lead}
      \end{figure}

      Policy iteration, as detailed by equations (\ref{eqn:update-value-single-agent}) and (\ref{eqn:update-policy-single-agent}), yields a policy $\pi^*$ optimizing rest-of-inning run expectancy. Tables \ref{tab:lead-by-count}, \ref{tab:lead-by-runners-outs} and \ref{tab:lead-by-players} report the optimal lead distance policy $\pi^*$ for different game states and player skills.
    
      Table \ref{tab:lead-by-count} shows how optimal lead distance varies by ball-strike count and number of prior disengagements, for an average runner facing an average battery (pitcher and catcher) with no outs. We observe that count has a mild influence on optimal lead distance, and the number of prior disengagements has a strong influence. As the pitcher nears the disengagement limit, the runner has more incentive to extend their lead distance because pickoff attempts are less likely and a free base advancement (from a third unsuccessful pickoff attempt) is more likely.
 
      \begin{table}[H]
        \centering
        \input{../../output/tables/lead_by_count.tex}
        \caption{
          \it The optimal lead distance (in feet) by count and prior disengagements for an average runner on first base, facing an average battery (pitcher and catcher) with no outs and third base empty. This lead distance policy maximizes the expected runs to end of inning as modeled by a the single-agent MDP.
        }
        \label{tab:lead-by-count}
      \end{table}
      
      Across counts, the recommended increase in lead distance is generally close to 1.5 feet after the first disengagement and 2.5 feet after the second disengagement. We observe this to be a good approximation across different runner/battery skills and different numbers of outs. Based on these results, we recommend the {\it Two-Foot Rule}: that runners increase their lead by two feet after each disengagement, a very simple and actionable guideline for athletes to follow.
    
      Table \ref{tab:lead-by-runners-outs} shows how optimal lead distance varies by number of outs, for an average runner facing an average battery with a 0-0 count. Again, we observe that each successive disengagement leads to an increase in recommended lead distance of approximately two feet. The recommended leads for zero outs and one out are virtually identical, and for two outs they are slightly longer. With two outs, stolen bases are more likely and pickoff attempts are less likely, so runners can be more aggressive.

      \begin{table}[H]
        \centering
        \input{../../output/tables/lead_by_outs.tex}
        \caption{
          \it The optimal lead distance (in feet) by outs and prior disengagements for an average runner on first base, facing an average battery (pitcher and catcher) with an 0-0 count and third base empty. This lead distance policy maximizes the expected runs to end of inning as modeled by the single-agent MDP.
        }
        \label{tab:lead-by-runners-outs}
      \end{table}

      Table \ref{tab:lead-by-players} shows how optimal lead distance varies by battery and runner skill, with no outs and a 0-0 count. The battery skill accounts for catcher arm strength and for catcher and pitcher random effects. The composite runner skill accounts for sprint speed and for runner random effect. Against batteries more skilled at controlling the run game, the optimal lead distance is shorted. For more skilled runners, the optimal lead distance is longer. Regardless of the skill levels, the optimal policy for the runner is to increase their lead by 1.5--2.5 feet with each successive disengagement. We observe that the Two-Foot Rule is a good approximation for different combinations of battery and runner skill, not just for average players.
    
      \begin{table}[H]
        \centering
        \input{../../output/tables/lead_by_players.tex}
        \caption{
          \it Optimal lead distance (in feet) by battery/runner skill and prior disengagements, for an 0-0 count with no outs and third base empty. Player skill is held constant across runner outcome models; for example, the hypothetical 90$^{th}$-percentile battery represents the 90$^{th}$ percentile at attempting pickoffs, successfully executing pickoffs, suppressing stolen base attempts, and catching would-be base stealers.
        }
        \label{tab:lead-by-players}
      \end{table}

      Table \ref{tab:actual-vs-rec} compares observed runner behavior against the optimal policy learned by the MDP. For each pitch with a runner on first base and second base empty, we compare the runner's lead distance with the recommended lead distance for an average runner against an average battery. With zero disengagements, the average lead is slightly less than the average recommended lead, and most leads are shorter than recommended. As the disengagements increase, this gap widens. On average, batters increase their leads by 0.6--0.7 feet afer each disengagement, significantly less than the Two-Foot Rule would recommend.

      \begin{table}[H]
        \centering
        \input{../../output/tables/actual_vs_rec_lead.tex}
        \caption{
          \it Comparison of actual versus recommended lead distance by number of prior disengagements. The recommended lead distance is based on the single-agent MDP model and accounts for count, outs and prior disengagements, assuming average battery and runner skill.
        }
        \label{tab:actual-vs-rec}
      \end{table}

      Our recommendations differ from runner behavior, and we can approximate the difference in run value between runner behavior and our recommendations. In 2023, the scoring expectation was 0.516 runs per inning. Under the optimal lead distance policy $\pi^*$, the corresponding value of the start-of-inning state is 0.528 runs, an increase of 0.012 runs per inning. While that may seem insigificant at first, it adds up to approximately 17 runs over 162 games of nine innings each. To add that many expected runs in free agency would cost roughly \$17 million \parencite{clemens_what_2021}, which speaks to a significant (but not outlandish) impact.

    \subsection{Two-player stochastic game}

      All of the results shown so far correspond to our single-agent model, in which the runner chooses a lead distance and all other outcomes are probabilistic based on their choice. While the single-agent model is the most immediately useful framework for runner decision-making in the current environment, we also consider how pitchers might respond using the same model to optimize their decision-making. In the two-agent model, the runner first chooses a lead distance, and the pitcher then decides whether to attempt a pickoff, as detailed in Section \ref{sec:sequential-model}. The optimal strategy for the runner is maximin because they choose the lead distance which maximizes the minimum run expectancy across the pitcher's two options (pickoff or pitch).

      In the two-agent model, pitchers would be more aggressive with attempting pickoffs when runners take long leads. Based on observed lead distances in 2023, we estimate that a pickoff attempt would reduce run expectancy relative to a pitch in 8.3\% of opportunities with only first base occupied. When a pickoff attempt would have been recommended, pitchers attempted pickoffs 10.8\% of the time---this would be 100\% in the two-agent model. When a pickoff attempt would not have been recommended, pitchers attempted pickoffs 5.0\% of the time---this would be 0\% in the two-agend model. Overall, pitchers attempted pickoffs on 5.8\% of opportunities, less than the recommmended 8.3\%.

      Table \ref{tab:count-twoagent} reports the optimal lead distance policy by count when both runner and pitcher are behaving optimally under the two-agent model. In this setting, the optimal lead distance is longer than in the single-agent model (Table \ref{tab:lead-by-count}), especially early in the count and with two prior disengagements. The longer lead distances are somewhat counterintuitive because we expect more pickoff attempts from pitchers when behaving optimally. The explanation is that in the two-agent model, the pitcher will never attempt a pickoff when it is suboptimal to do so. The optimal behavior for the runner is to force their hand (or rather, to make the pitcher indifferent to their two choices). Interestingly, the Two-Foot Rule breaks down in this setting. Averaged across counts, we recommend extending the lead by roughly 0.7 feet after the first disengagement and by much more after the second disengagement---roughly 4.2 feet! However, this advice will only be actionable if pitchers change their behavior to be more in line with the modeled optimal policy.
      
      \begin{table}[H]
        \centering
        \input{../../output/tables/count_two_agent.tex}
        \caption{
          \it Based on the count in each row, each entry shows the optimal lead distance (in feet) for a runner on 1B depending on the number of disengagements for our two-agent model.
        }
        \label{tab:count-twoagent}
      \end{table}

      If both runners and pitchers adapted their behavior to match the game theoretic optimal policy under the two-agent model, the corresponding run value of the start-of-inning state is 0.520 runs per inning. Relative to the current status quo, this reflects an increase of 0.004 runs per inning, meaning a very slight increase to overall offense. Over 162 games of nine innings each, we would expect each team to score approximately 5 more runs over a full season.

  \section{Discussion}

    Under the new pickoff rules, baseball players and coaches understand intuitively that runners want to increase their lead distance after each successive disengagement by the pitcher---but by how much? In this paper, we formalize the cat-and-mouse game between pitcher and runner as a Markov decision process and estimate the optimal lead distance policy for a runner on first base with other bases empty, given the number of outs, the ball-strike count, and the number of prior disengagements by the pitcher. We model the probabilities of runner outcomes (pickoff attempt/success, stolen base attempt/success) as functions of lead distance, context, and player skill. This allows us to estimate the optimal lead distance policy for any combination of pitcher skill and runner skill.

    First, we consider a single-agent model in which the runner has agency in deciding their lead distance, and the pitcher's decision of whether to attempt a pickoff is probabilistic, conditioned on the lead distance. This framework leads to the most actionable recommendation for runners under the current conditions of pitcher behavior. We estimate that the average team could expect to increase their expected offensive output by approximately 17 runs over a full season by implementing the optimal lead distance policy. Based on these results, we propose the Two-Foot Rule: We recommend that a runner on first base increase their lead distance by two feet after each successive disengagement. This easily implemented recommendation is a good approximation to the optimal lead distance policy across a wide array of contexts (outs and ball-strike count) and pitcher/runner skills.

    We go a step further and consider the consequences of granting the pitcher agency to decide whether to attempt a pickoff given the runner's lead distance. We give a game theoretic treatment to this two-agent model in which the runner follows a maximin strategy (with respect to rest-of-inning run expectancy) so that the pitcher is indifferent to attempting a pickoff or throwing a pitch. Under the game theoretic equilibrium, runners are more aggressive with their lead distance and pitchers are more aggressive with their pickoff attempts than the current behavior we observe. While baseball analytics sometimes garners negative attention for diminishing the aesthetic appeal of the game \parencite{sheinin_analytics_2025}, this is an example for which the analytically recommended strategy could be more aesthetically pleasing.

    Our results come with some simplifying assumptions that are necessary limitations given the data available. We assume that observed lead distance fully describes the runner's behavior and that they cannot disguise their aggressiveness by leaning in one direction. In reality, the runner may achieve a greater (or lesser) effective lead distance than we observed by leaning toward second base (or first base). We also assume that lead distance only impacts pickoff and stolen base outcomes, not the outcomes of batted balls. For example, we assume that a greater lead distance does not improve the runner's probability of advancing from first base to third base on a single. Baseball domain knowledge suggests this is a reasonable assumption because while a pitch is being thrown, the runner moves to a secondary lead distance which is more important for determining advancement outcomes than primary lead distance (at first pitcher movement). Finally, we assume that, other than pickoff attempts, no other disengagements occur. In reality, the pitcher may derive value from stepping off of the pitching rubber if they are short of breath or need to resolve a miscommunication with the catcher.

    In addition to the lead distance data used in the current work, which is based on the runner's center of mass, MLB has begun collecting 18-point pose tracking data on runners and fielders at 50 frames per second \parencite{jedlovec_introducing_2020}. If these data were publicly available, they would enable more granular modeling of the cat-and-mouse game between pitcher and runner. For example, with information about the runner's leaning posture and reaction time, future work might show that a mixed strategy for the pitcher's pickoff attempt decision achieves the best result by forcing a neutral posture for the runner and delaying the runner's reaction. For now, we present the Two-Foot Rule as a simple rule of thumb that provides easily digestible and actionable guidance to runners.

  \section*{Acknowledgments}

    Major League Baseball trademarks and copyrights are used with permission of MLB Advanced Media, L.P. All rights reserved.

  \section*{Code and Data Availability}

    All code related to this paper is available at \url{https://github.com/jfhahn2/pickoff-game-theory}. Lead distance data are not publicly available and were obtained from Major League Baseball. All other data are publicly available and may be downloaded using the R package sabRmetrics \parencite{powers_sabrmetrics_2025}.

  \printbibliography

\end{document}
